{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Guidelines :\n",
    "# 1. Convert every string to lower-case\n",
    "# 2. Remove any punctuation, i.e., any character other than [a...z]\n",
    "# 3. For each sub-question submit 2 files: \n",
    "# text file with the results (any format you believe is more suitable) and the python code\n",
 
    "\n",
    "\n",
    "# http://spark.apache.org/docs/latest/programming-guide.html \n",
    "# https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#mapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a) For each word in the input text, you must compute the occurrences of other words appear in every line that it appeared. \n",
    "# For example, assume that we have the following line as an input text.\n",
    "# “Theseus seemed to me to resemble Romulus in many particulars.”\n",
    "# With the word “Theseus”: “to” occurs twice, “seemed” occurs once, “me” occurs once and so on.\n",
    "# With the word “to”, “Theseus” occurs once, “to” occurs once (not zero times!), and so on.\n",
    "# For this assignment, you are going to compute the occurrences of every word for all lines, not just for a single line.\n",
    "\n",
    "\n",
    "txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg674.txt\")\n",
    "# txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg.txt\")\n",
    "import string\n",
    "\n",
    "def func_map(line):\n",
    "    line = line.translate(str.maketrans( '', '', string.punctuation)).lower().split()\n",
    "    list_pairs=[]    \n",
    "    list_unique=[]\n",
    "    for i in range(len(line)) : \n",
    "        key_a =line[i]\n",
    "        if key_a in list_unique:\n",
    "            continue\n",
    "        else:\n",
    "            list_unique.append(key_a)\n",
    "        for j in range(len(line)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            key_b = line[j]\n",
    "            list_pairs.append(((key_a,key_b),1))\n",
    "    return list_pairs\n",
    "    \n",
    "txtmap = txt.flatMap(func_map)\n",
    "# txtmap.collect()\n",
    "\n",
    "txtreduce = txtmap.reduceByKey(lambda a, b: a + b)\n",
    "# txtreduce.collect()\n",
    "\n",
    "txtmap_map = txtreduce.map(lambda line: (line[0][0], (line[0][1],line[1]))).sortByKey()\n",
    "# txtmap_map.collect()\n",
    "txtreduce_reduce = txtmap_map.groupByKey().mapValues(list).sortByKey()\n",
    "# txtreduce_reduce.mapValues(list).collect()\n",
    "txtreduce_reduce.saveAsTextFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/result_A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# b) Produce a list of words, with the top-10 words that appear with this word for all lines.\n",
    "\n",
    "txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg674.txt\")\n",
    "# txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg.txt\")\n",
    "import string\n",
    "\n",
    "def func_map(line):\n",
    "    line = line.translate(str.maketrans( '', '', string.punctuation)).lower().split()\n",
    "    list_pairs=[]    \n",
    "    list_unique=[]\n",
    "    for i in range(len(line)) : \n",
    "        key_a =line[i]\n",
    "        if key_a in list_unique:\n",
    "            continue\n",
    "        else:\n",
    "            list_unique.append(key_a)\n",
    "        for j in range(len(line)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            key_b = line[j]\n",
    "            list_pairs.append(((key_a,key_b),1))\n",
    "    return list_pairs\n",
    "    \n",
    "txtmap = txt.flatMap(func_map)\n",
    "# txtmap.collect()\n",
    "\n",
    "txtreduce = txtmap.reduceByKey(lambda a, b: a + b)\n",
    "# txtreduce.collect()\n",
    "\n",
    "txtmap_map = txtreduce.map(lambda line: (line[0][0], (line[0][1],line[1])))\n",
    "# txtmap_map.collect()\n",
    "\n",
    "def sort_list(x):\n",
    "    top_k = 3\n",
    "    return sorted(x, key = lambda item: item[1],reverse=True)[0:top_k]\n",
    "txtreduce_reduce = txtmap_map.groupByKey().mapValues(sort_list).sortByKey()\n",
    "txtreduce_reduce.mapValues(list).collect()\n",
    "txtreduce_reduce.saveAsTextFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/result_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c) For each word in the input text, you must compute the occurrences of other words that appear in every line that it appeared, \n",
    "# right after the appearance of the word. For example, assume that we have the following line as an input text\n",
    "# “Theseus seemed to me to resemble Romulus in many particulars.”\n",
    "# With the word “Theseus”, “seemed” occurs once. With the word “seemed”, “to” occurs once.\n",
    "# Produce a list of words, with the top-10 words that appear right after this word for all lines.\n",
    "\n",
    "\n",
    "txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg674.txt\")\n",
    "# txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg.txt\")\n",
    "import string\n",
    "\n",
    "def func_map(line):\n",
    "    line = line.translate(str.maketrans( '', '', string.punctuation)).lower().split()\n",
    "    list_pairs=[]  \n",
    "    for i in range(len(line)) :\n",
    "        if i +1 == len(line):\n",
    "            break\n",
    "        j = i + 1\n",
    "        key_a =line[i]\n",
    "        key_b = line[j]\n",
    "        list_pairs.append(((key_a,key_b),1))\n",
    "    return list_pairs\n",
    "    \n",
    "txtmap = txt.flatMap(func_map)\n",
    "# txtmap.collect()\n",
    "\n",
    "txtreduce = txtmap.reduceByKey(lambda a, b: a + b)\n",
    "# txtreduce.collect()\n",
    "\n",
    "txtmap_map = txtreduce.map(lambda line: (line[0][0], (line[0][1],line[1])))\n",
    "# txtmap_map.collect()\n",
    "\n",
    "def sort_list(x):\n",
    "    top_k = 10\n",
    "    return sorted(x, key = lambda item: item[1],reverse=True)[0:top_k]\n",
    "txtreduce_reduce = txtmap_map.groupByKey().mapValues(sort_list).sortByKey()\n",
    "# txtreduce_reduce.mapValues(list).collect()\n",
    "txtreduce_reduce.saveAsTextFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/result_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d) Generalize step (c) so that you include all words that appear up to distance k after a given word.\n",
    "\n",
    "txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg674.txt\")\n",
    "# txt = sc.textFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/pg.txt\")\n",
    "import string\n",
    "\n",
    "def func_map(line):\n",
    "    line = line.translate(str.maketrans( '', '', string.punctuation)).lower().split()\n",
    "    list_pairs=[]\n",
    "    k = 3\n",
    "    for i in range(len(line)) :\n",
    "        if i+k > len(line):\n",
    "            end =  len(line)\n",
    "        else:\n",
    "            end = i+k\n",
    "        key_a =line[i]\n",
    "        for j in range(i +1,end):\n",
    "            key_b = line[j]\n",
    "            list_pairs.append(((key_a,key_b),1))\n",
    "    return list_pairs\n",
    "    \n",
    "txtmap = txt.flatMap(func_map).sortByKey()\n",
    "# txtmap.collect()\n",
    "\n",
    "txtreduce = txtmap.reduceByKey(lambda a, b: a + b)\n",
    "# txtreduce.collect()\n",
    "\n",
    "txtmap_map = txtreduce.map(lambda line: (line[0][0], (line[0][1],line[1])))\n",
    "# txtmap_map.collect()\n",
    "\n",
    "def sort_list(x):\n",
    "    top_k = 10\n",
    "    return sorted(x, key = lambda item: item[1],reverse=True)[0:top_k]\n",
    "txtreduce_reduce = txtmap_map.groupByKey().mapValues(sort_list).sortByKey()\n",
    "txtreduce_reduce.mapValues(list).collect()\n",
    "txtreduce_reduce.saveAsTextFile(\"/Users/nanazhu/Documents/LosAlamos/BigDataComputing/Spark/result_D\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3.5]",
   "language": "python",
   "name": "conda-env-Python3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
